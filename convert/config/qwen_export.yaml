# Qwen3-VL Model Export Configuration

model:
  name: "Qwen/Qwen3-VL-8B-Thinking"
  use_fp16: true  # Use FP16 to reduce memory (16GB instead of 32GB)

onnx:
  opset_version: 18  # Same as SigLIP (latest stable)
  optimize: true
  dynamic_batch: true
  max_seq_length: 512  # Limit for initial implementation

  # Split export strategy
  export_vision_encoder: true
  export_text_decoder: true

  dynamic_axes:
    # Vision inputs
    pixel_values:
      0: batch_size
      2: height
      3: width
    # Text inputs
    input_ids:
      0: batch_size
      1: sequence_length
    attention_mask:
      0: batch_size
      1: sequence_length

tensorrt:
  fp16: true
  int8: true  # Enable INT8 for 8B model
  workspace_gb: 8  # Larger workspace for big model

  # Batch configuration
  min_batch: 1
  opt_batch: 1
  max_batch: 4  # Smaller max batch due to model size

validation:
  num_test_samples: 5  # Fewer samples (generation is slow)
  max_new_tokens: 128  # Limit for testing

benchmark:
  warmup_iterations: 5
  benchmark_iterations: 20
  batch_sizes: [1, 2]  # Limited due to model size

# Expected Performance (NVIDIA GPU):
# - PyTorch (4-bit): ~2-3s per image (baseline)
# - ONNX (FP16): ~3-4s per image (dequantization overhead)
# - TensorRT (INT8): ~1.5-2s per image (best case)
